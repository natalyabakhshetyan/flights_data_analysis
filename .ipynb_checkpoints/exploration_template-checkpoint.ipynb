{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flights Data Exploration\n",
    "## by Natalya Bakhshetyan\n",
    "\n",
    "## Preliminary Wrangling\n",
    "\n",
    "> This document explores flight data for years 2017 through 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all packages and set plots to be embedded inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the datasets into pandas DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(583985, 25) (570118, 25) (450017, 25)\n"
     ]
    }
   ],
   "source": [
    "flight_data1 = pd.read_csv(\"flight_data1.csv\", dtype = {'DEP_TIME': object, 'ARR_TIME': object})\n",
    "flight_data2 = pd.read_csv(\"flight_data2.csv\", dtype = {'DEP_TIME': object, 'ARR_TIME': object})\n",
    "flight_data3 = pd.read_csv(\"flight_data3.csv\", dtype = {'DEP_TIME': object, 'ARR_TIME': object})\n",
    "print(flight_data1.shape, flight_data2.shape, flight_data3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample 10,000 rows from each DataFrame for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data1 = flight_data1.sample(n=10000, random_state=1)\n",
    "sample_data2 = flight_data2.sample(n=10000, random_state=1)\n",
    "sample_data3 = flight_data3.sample(n=10000, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine all 3 DataFrames into 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat([sample_data1, sample_data2, sample_data3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 25)\n",
      "YEAR                     int64\n",
      "QUARTER                  int64\n",
      "MONTH                    int64\n",
      "DAY_OF_WEEK              int64\n",
      "ORIGIN                  object\n",
      "ORIGIN_CITY_NAME        object\n",
      "ORIGIN_STATE_NM         object\n",
      "DEST                    object\n",
      "DEST_CITY_NAME          object\n",
      "DEST_STATE_NM           object\n",
      "DEP_TIME                object\n",
      "DEP_DELAY_NEW          float64\n",
      "ARR_TIME                object\n",
      "ARR_DELAY_NEW          float64\n",
      "CANCELLED              float64\n",
      "DIVERTED               float64\n",
      "AIR_TIME               float64\n",
      "FLIGHTS                float64\n",
      "DISTANCE               float64\n",
      "CARRIER_DELAY          float64\n",
      "WEATHER_DELAY          float64\n",
      "NAS_DELAY              float64\n",
      "SECURITY_DELAY         float64\n",
      "LATE_AIRCRAFT_DELAY    float64\n",
      "Unnamed: 24            float64\n",
      "dtype: object\n",
      "Index(['YEAR', 'QUARTER', 'MONTH', 'DAY_OF_WEEK', 'ORIGIN', 'ORIGIN_CITY_NAME',\n",
      "       'ORIGIN_STATE_NM', 'DEST', 'DEST_CITY_NAME', 'DEST_STATE_NM',\n",
      "       'DEP_TIME', 'DEP_DELAY_NEW', 'ARR_TIME', 'ARR_DELAY_NEW', 'CANCELLED',\n",
      "       'DIVERTED', 'AIR_TIME', 'FLIGHTS', 'DISTANCE', 'CARRIER_DELAY',\n",
      "       'WEATHER_DELAY', 'NAS_DELAY', 'SECURITY_DELAY', 'LATE_AIRCRAFT_DELAY',\n",
      "       'Unnamed: 24'],\n",
      "      dtype='object')\n",
      "        YEAR  QUARTER  MONTH  DAY_OF_WEEK ORIGIN ORIGIN_CITY_NAME  \\\n",
      "380728  2019        1      1            3    MDW      Chicago, IL   \n",
      "544669  2019        1      1            3    DEN       Denver, CO   \n",
      "211691  2019        1      1            3    MDW      Chicago, IL   \n",
      "501386  2019        1      1            3    ATL      Atlanta, GA   \n",
      "443541  2019        1      1            3    LGA     New York, NY   \n",
      "\n",
      "       ORIGIN_STATE_NM DEST          DEST_CITY_NAME DEST_STATE_NM  ...  \\\n",
      "380728        Illinois  DTW             Detroit, MI      Michigan  ...   \n",
      "544669        Colorado  MKE           Milwaukee, WI     Wisconsin  ...   \n",
      "211691        Illinois  MSP         Minneapolis, MN     Minnesota  ...   \n",
      "501386         Georgia  IND        Indianapolis, IN       Indiana  ...   \n",
      "443541        New York  SRQ  Sarasota/Bradenton, FL       Florida  ...   \n",
      "\n",
      "       DIVERTED  AIR_TIME FLIGHTS  DISTANCE  CARRIER_DELAY  WEATHER_DELAY  \\\n",
      "380728      0.0      45.0     1.0     228.0           38.0            0.0   \n",
      "544669      0.0     123.0     1.0     896.0            NaN            NaN   \n",
      "211691      0.0      57.0     1.0     349.0            0.0            0.0   \n",
      "501386      0.0      57.0     1.0     432.0           91.0            0.0   \n",
      "443541      0.0     147.0     1.0    1047.0            NaN            NaN   \n",
      "\n",
      "        NAS_DELAY  SECURITY_DELAY  LATE_AIRCRAFT_DELAY  Unnamed: 24  \n",
      "380728        7.0             0.0                  0.0          NaN  \n",
      "544669        NaN             NaN                  NaN          NaN  \n",
      "211691        0.0             0.0                107.0          NaN  \n",
      "501386        0.0             0.0                  5.0          NaN  \n",
      "443541        NaN             NaN                  NaN          NaN  \n",
      "\n",
      "[5 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "# high-level overview of data shape and composition\n",
    "print(combined_df.shape)\n",
    "print(combined_df.dtypes)\n",
    "print(combined_df.columns)\n",
    "print(combined_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#convert DAY_OF_WEEK to ordered categorical\n",
    "\n",
    "def to_categorical(col, ordered_categories):\n",
    "    ordered_var = pd.api.types.CategoricalDtype(ordered = True, categories = ordered_categories)\n",
    "    combined_df[col] = combined_df[col].astype(ordered_var)\n",
    "\n",
    "col = \"DAY_OF_WEEK\"\n",
    "ordered_categories = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', \"Saturday\", 'Sunday']\n",
    "combined_df[col] = combined_df[col].astype(str)\n",
    "weekday_dict = {'1': 'Monday', '2' : \"Tuesday\", '3': 'Wednesday',\n",
    "               '4': 'Thursday', '5': 'Friday', '6': 'Saturday',\n",
    "               '7': 'Sunday'}\n",
    "combined_df[col] = combined_df[col].replace(weekday_dict)\n",
    "to_categorical(col, ordered_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Wednesday, Sunday, Tuesday, Monday, Saturday, Thursday, Friday]\n",
       "Categories (7, object): [Monday < Tuesday < Wednesday < Thursday < Friday < Saturday < Sunday]"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#confirm conversion of DAY_OF_WEEK to ordered categorical\n",
    "\n",
    "combined_df.DAY_OF_WEEK.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29211, 25)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#drop rows containing null values in columns DEP_TIME and ARR_TIME\n",
    "combined_df.dropna(subset = ['ARR_TIME'], how = 'all', inplace = True)\n",
    "combined_df.dropna(subset = ['DEP_TIME'], how = 'all', inplace = True)\n",
    "#confirm the change\n",
    "combined_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "380728    14:43:00\n",
      "544669    15:11:00\n",
      "211691    00:04:00\n",
      "501386    23:03:00\n",
      "443541    10:09:00\n",
      "403423    14:19:00\n",
      "263383    18:58:00\n",
      "131232    13:33:00\n",
      "439250    20:46:00\n",
      "199955    08:52:00\n",
      "388695    15:20:00\n",
      "106354    10:18:00\n",
      "303020    08:48:00\n",
      "340065    17:24:00\n",
      "370913    06:44:00\n",
      "110145    17:07:00\n",
      "237786    08:29:00\n",
      "30967     14:36:00\n",
      "494159    13:22:00\n",
      "392647    15:18:00\n",
      "150369    08:22:00\n",
      "222944    17:25:00\n",
      "48271     14:41:00\n",
      "543135    17:13:00\n",
      "114230    05:50:00\n",
      "486664    09:22:00\n",
      "473541    20:33:00\n",
      "357885    19:41:00\n",
      "576937    06:53:00\n",
      "2533      06:43:00\n",
      "            ...   \n",
      "429762    12:20:00\n",
      "273022    17:15:00\n",
      "55266     15:09:00\n",
      "45742     12:12:00\n",
      "56030     15:07:00\n",
      "407867    05:53:00\n",
      "94964     20:52:00\n",
      "362662    09:43:00\n",
      "185084    20:53:00\n",
      "127578    22:22:00\n",
      "192806    20:16:00\n",
      "308704    11:39:00\n",
      "218331    10:20:00\n",
      "75638     15:22:00\n",
      "379280    11:34:00\n",
      "332375    08:19:00\n",
      "337191    08:36:00\n",
      "436654    13:44:00\n",
      "287967    09:40:00\n",
      "232366    13:30:00\n",
      "384548    05:51:00\n",
      "287945    12:07:00\n",
      "175727    21:01:00\n",
      "115867    17:07:00\n",
      "232507    08:34:00\n",
      "312437    12:32:00\n",
      "212301    18:23:00\n",
      "197262    09:58:00\n",
      "6487      20:00:00\n",
      "432779    15:11:00\n",
      "Name: DEP_TIME, Length: 29211, dtype: object\n",
      "380728    16:55:00\n",
      "544669    18:52:00\n",
      "211691    01:12:00\n",
      "501386    00:31:00\n",
      "443541    13:54:00\n",
      "Name: ARR_TIME, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#convert DEP_TIME and ARR_TIME from float to datetime\n",
    "def float_to_datetime(col):\n",
    "    combined_df[col] = combined_df[col].apply(lambda x: x.zfill(4))\n",
    "    combined_df[col] = pd.to_datetime(combined_df[col], format='%H%M', errors = 'coerce').dt.time\n",
    "    \n",
    "float_to_datetime('DEP_TIME')\n",
    "float_to_datetime('ARR_TIME')\n",
    "print(combined_df.DEP_TIME.head())\n",
    "print(combined_df.ARR_TIME.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29192, 25)"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#drop rows containing null values in columns DEP_TIME and ARR_TIME\n",
    "combined_df.dropna(subset = ['ARR_TIME'], how = 'all', inplace = True)\n",
    "combined_df.dropna(subset = ['DEP_TIME'], how = 'all', inplace = True)\n",
    "#confirm the change\n",
    "combined_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the structure of your dataset?\n",
    "\n",
    "> Your answer here!\n",
    "\n",
    "### What is/are the main feature(s) of interest in your dataset?\n",
    "\n",
    "> Your answer here!\n",
    "\n",
    "### What features in the dataset do you think will help support your investigation into your feature(s) of interest?\n",
    "\n",
    "> Your answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate Exploration\n",
    "\n",
    "> In this section, investigate distributions of individual variables. If\n",
    "you see unusual points or outliers, take a deeper look to clean things up\n",
    "and prepare yourself to look at relationships between variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Make sure that, after every plot or related series of plots, that you\n",
    "include a Markdown cell with comments about what you observed, and what\n",
    "you plan on investigating next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discuss the distribution(s) of your variable(s) of interest. Were there any unusual points? Did you need to perform any transformations?\n",
    "\n",
    "> Your answer here!\n",
    "\n",
    "### Of the features you investigated, were there any unusual distributions? Did you perform any operations on the data to tidy, adjust, or change the form of the data? If so, why did you do this?\n",
    "\n",
    "> Your answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bivariate Exploration\n",
    "\n",
    "> In this section, investigate relationships between pairs of variables in your\n",
    "data. Make sure the variables that you cover here have been introduced in some\n",
    "fashion in the previous section (univariate exploration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Talk about some of the relationships you observed in this part of the investigation. How did the feature(s) of interest vary with other features in the dataset?\n",
    "\n",
    "> Your answer here!\n",
    "\n",
    "### Did you observe any interesting relationships between the other features (not the main feature(s) of interest)?\n",
    "\n",
    "> Your answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate Exploration\n",
    "\n",
    "> Create plots of three or more variables to investigate your data even\n",
    "further. Make sure that your investigations are justified, and follow from\n",
    "your work in the previous sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Talk about some of the relationships you observed in this part of the investigation. Were there features that strengthened each other in terms of looking at your feature(s) of interest?\n",
    "\n",
    "> Your answer here!\n",
    "\n",
    "### Were there any interesting or surprising interactions between features?\n",
    "\n",
    "> Your answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> At the end of your report, make sure that you export the notebook as an\n",
    "html file from the `File > Download as... > HTML` menu. Make sure you keep\n",
    "track of where the exported file goes, so you can put it in the same folder\n",
    "as this notebook for project submission. Also, make sure you remove all of\n",
    "the quote-formatted guide notes like this one before you finish your report!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
